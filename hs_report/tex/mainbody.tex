%=================================================================
\section{Introduction}
Ordinal regression was conducted on a dataset derived from a deep learning model trained on the quality dataset of red variants of the "Vinho Verde" wine from Spain. This dataset characterizes the impact of various chemical substances present in wine on its quality. The quality grades are ordinal and imbalanced, with common wines being significantly more prevalent than either high-quality or low-quality wines.

\section{Preliminaries} 
\subsection{Feature}\phantom{...}

In order to facilitate an understanding of the meanings of various data points in the dataset, it is necessary to provide a brief introduction to several features of wine that are relevant to the dataset.
\begin{itemize}
	\item \textbf{Fixed Acidity}: Most acids involved with wine are fixed or nonvolatile (do not evaporate readily).
	\item \textbf{Volatile Acidity}: The amount of acetic acid in wine, which at too high levels can lead to an unpleasant, vinegar taste.
	\item \textbf{Citric Acid}: Found in small quantities, citric acid can add 'freshness' and flavor to wines.
	\item \textbf{Residual Sugar}: The amount of sugar remaining after fermentation stops. Wines with less than 1 gram/liter are rare, and those with more than 45 grams/liter are considered sweet.
	\item \textbf{Chlorides}: The amount of salt in the wine.
	\item \textbf{Free Sulfur Dioxide}: The free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine.
	\item \textbf{Total Sulfur Dioxide}: Amount of free and bound forms of SO2. In low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine.
	\item \textbf{Density}: The density of wine is close to that of water, depending on the percent alcohol and sugar content.
	\item \textbf{pH}: Describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale.
	\item \textbf{Sulphates}: A wine additive that can contribute to sulfur dioxide gas (SO2) levels, acting as an antimicrobial and antioxidant.
	\item \textbf{Alcohol}: The percent alcohol content of the wine.
	\item \textbf{Quality}: Wine quality rating.
\end{itemize}
\subsection{SMOTE}\phantom{...}

In the provided dataset, the samples with quality grades 5 and 6 are substantially more numerous than those of other grades, necessitating a consideration of the potential impacts of employing the SMOTE technique.

SMOTE (Synthetic Minority Over-sampling Technique) is a method employed in data science and machine learning to address the issue of class imbalance in classification problems. Class imbalance refers to the scenario where the instance count of one class (the minority class) is significantly lower than that of other classes (the majority classes). This imbalance can lead to biased performance or suboptimal results in machine learning models, as they are often dominated by the majority class and tend to overlook the minority class. 

The operational mechanism of SMOTE involves:

Sample Selection: Initially, SMOTE selects a sample from the minority class.
Neighbor Identification: It then identifies the k-nearest neighbors of this sample in the feature space, where k is typically a small integer.
Synthesis of New Samples: For each selected minority class sample, SMOTE generates synthetic samples. This is achieved through linear interpolation in the feature space between the selected sample and its chosen neighbor.
Repetition of the Process: This process is repeated until the desired class balance is achieved.

Key aspects of SMOTE include:

Over-sampling Technique: It is an over-sampling approach, as opposed to under-sampling, which involves reducing the number of majority class samples.

Creation of Synthetic Samples: SMOTE generates new, synthetic samples rather than merely duplicating existing ones. This contributes to enhancing the diversity of the dataset.

Mitigating Overfitting: By generating synthetic samples, SMOTE aids in reducing the overfitting issues that simple over-sampling might cause.


\subsection{KAPPA}\phantom{...}

Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.

The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix O is constructed, such that Oi,j corresponds to the number of Ids i (actual) that received a predicted value j. An N-by-N matrix of weights, w, is calculated based on the difference between actual and predicted values:

\[ w_{i,j} = \frac{(i - j)^2}{(N - 1)^2} \]

An N-by-N histogram matrix of expected outcomes, E, is calculated assuming that there is no correlation between values.  This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that E and O have the same sum.

From these three matrices, the quadratic weighted kappa is calculated as: 

\[ \kappa = 1 - \frac{\sum_{i,j} w_{i,j} O_{i,j}}{\sum_{i,j} w_{i,j} E_{i,j}} \]

\section{Experiment and analysis}
\subsection{Dataset Analysis}\phantom{...}

Analysis of the Original Dataset, Training Dataset, and Testing Dataset

The datasets do not contain any missing values, thus negating the necessity for missing value processing.






\section{Experiment and Analysis} 



\section{Conclusions} 



\section*{Acknowledgement}


The authors would like to thank \ldots

